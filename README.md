# ML-TestingTools
The following reposity contians a collection of tools to test and validate machine learning models and datasets


## Model Analysis

### [Yellowbrick](https://www.scikit-yb.org/en/latest/)
Yellowbrick is a Python library that provides visualizations for model selection, evaluation, and validation. It includes tools for generating confusion matrices, learning curves, ROC curves, and other diagnostic plots. It can be used with various machine learning frameworks, such as scikit-learn and TensorFlow.

Visualizers:
- Feature Visualization
  - Rank Features
  - PCA Projections
  - ...
- Regression Visualization
  - Prediction Error Plot
  - Residuals Plot
  - ...
- Model Selection Visualization
  - Learning Curve
  - Feature Importance
  - ...



<br>

## Data Handling
[Great Expectations](https://greatexpectations.io/) - Great Expectations is an open-source framework for data validation that can be integrated with various machine learning frameworks. It provides tools for defining data expectations, validating data against those expectations, and generating data documentation.

Databricks MLflow - MLflow is an open-source platform for managing the machine learning lifecycle. It includes tools for tracking experiments, packaging code and models, and deploying models. The MLflow data validation component provides functions for schema validation and data quality checks.

Deequ - Deequ is an open-source library for data quality assessment and monitoring. It provides functions for profiling data, defining constraints, and validating data against those constraints. It can be integrated with Apache Spark for distributed data processing.

Datahub - Datahub is an open-source platform for managing metadata and data assets. It provides tools for cataloging and documenting data, as well as functions for data validation and profiling. It can be integrated with various data processing frameworks, such as Apache Spark and Presto.

Apache Griffin - Apache Griffin is an open-source tool for data quality assessment and monitoring. It provides functions for profiling data, defining rules, and validating data against those rules. It can be integrated with Apache Spark for distributed data processing.
